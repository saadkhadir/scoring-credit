from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Any, Optional, Union
import mlflow
from mlflow.tracking import MlflowClient
import pandas as pd
import numpy as np
import os
from datetime import datetime
import logging
import joblib
import pickle
import warnings

# Ignorer les warnings de compatibilitÃ©
warnings.filterwarnings('ignore')

# ==================== CONFIGURATION ====================
MLFLOW_TRACKING_URI = "sqlite:///mlflow.db"
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

# Chemin vers le modÃ¨le dans Docker - essayer plusieurs emplacements
MODEL_PATHS = [
    "/app/model/model.pkl",
    "/app/model",  # Dossier complet pour MLflow
    "/app/mlruns",  # Fallback vers mlruns
]

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== FASTAPI APP ====================
app = FastAPI(
    title="Credit Score ML API",
    description="API de production pour prÃ©diction de crÃ©dit avec MLflow",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS pour permettre les requÃªtes depuis un frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, spÃ©cifier les domaines autorisÃ©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== MODELS PYDANTIC ====================
class CreditApplicationInput(BaseModel):
    """
    Format d'entrÃ©e pour une demande de crÃ©dit individuelle.
    Les donnÃ©es sont au format ORIGINAL (non prÃ©traitÃ©es).
    """
    duration_in_month: int = Field(..., alias="Duration in month", ge=1, le=120)
    credit_amount: float = Field(..., alias="Credit amount", gt=0)
    installment_rate: int = Field(
        ..., 
        alias="Installment rate in percentage of disposable income",
        ge=1, le=4
    )
    age_in_years: int = Field(..., alias="Age in years", ge=18, le=100)
    num_existing_credits: int = Field(
        ..., 
        alias="Number of existing credits at this bank",
        ge=1, le=4
    )
    num_dependents: int = Field(
        ..., 
        alias="Number of people being liable to provide maintenance for",
        ge=1, le=2
    )
    
    # CatÃ©gorielles ordinales (codes A11, A12, etc.)
    status_checking_account: str = Field(
        ..., 
        alias="Status of existing checking account"
    )
    credit_history: str = Field(..., alias="Credit history")
    savings_account: str = Field(..., alias="Savings account/bonds")
    employment_since: str = Field(..., alias="Present employment since")
    job: str = Field(..., alias="Job")
    
    # CatÃ©gorielles nominales
    purpose: str = Field(..., alias="Purpose")
    personal_status_sex: str = Field(..., alias="Personal status and sex")
    other_debtors: str = Field(..., alias="Other debtors / guarantors")
    property: str = Field(..., alias="Property")
    other_installment_plans: str = Field(..., alias="Other installment plans")
    housing: str = Field(..., alias="Housing")
    telephone: str = Field(..., alias="Telephone")
    foreign_worker: str = Field(..., alias="foreign worker")
    
    class Config:
        populate_by_name = True
        json_schema_extra = {
            "example": {
                "Duration in month": 12,
                "Credit amount": 5000.0,
                "Installment rate in percentage of disposable income": 2,
                "Age in years": 35,
                "Number of existing credits at this bank": 1,
                "Number of people being liable to provide maintenance for": 1,
                "Status of existing checking account": "A12",
                "Credit history": "A32",
                "Savings account/bonds": "A61",
                "Present employment since": "A73",
                "Job": "A173",
                "Purpose": "A43",
                "Personal status and sex": "A93",
                "Other debtors / guarantors": "A101",
                "Property": "A121",
                "Other installment plans": "A143",
                "Housing": "A152",
                "Telephone": "A192",
                "foreign worker": "A201"
            }
        }
    
    def to_dataframe(self) -> pd.DataFrame:
        """Convertit l'input en DataFrame avec les noms de colonnes originaux"""
        data = {
            "Duration in month": self.duration_in_month,
            "Credit amount": self.credit_amount,
            "Installment rate in percentage of disposable income": self.installment_rate,
            "Age in years": self.age_in_years,
            "Number of existing credits at this bank": self.num_existing_credits,
            "Number of people being liable to provide maintenance for": self.num_dependents,
            "Status of existing checking account": self.status_checking_account,
            "Credit history": self.credit_history,
            "Savings account/bonds": self.savings_account,
            "Present employment since": self.employment_since,
            "Job": self.job,
            "Purpose": self.purpose,
            "Personal status and sex": self.personal_status_sex,
            "Other debtors / guarantors": self.other_debtors,
            "Property": self.property,
            "Other installment plans": self.other_installment_plans,
            "Housing": self.housing,
            "Telephone": self.telephone,
            "foreign worker": self.foreign_worker
        }
        return pd.DataFrame([data])


class PredictionResponse(BaseModel):
    """RÃ©ponse pour une prÃ©diction individuelle"""
    prediction: int = Field(..., description="0=Mauvais crÃ©dit, 1=Bon crÃ©dit")
    probability_good_credit: float = Field(..., description="ProbabilitÃ© de bon crÃ©dit (0-1)")
    probability_bad_credit: float = Field(..., description="ProbabilitÃ© de mauvais crÃ©dit (0-1)")
    risk_level: str = Field(..., description="Niveau de risque: LOW, MEDIUM, HIGH")
    model_version: str
    timestamp: str
    
    
class BatchPredictionRequest(BaseModel):
    """RequÃªte pour prÃ©dictions batch"""
    applications: List[CreditApplicationInput]
    

class BatchPredictionResponse(BaseModel):
    """RÃ©ponse pour prÃ©dictions batch"""
    predictions: List[PredictionResponse]
    total_processed: int
    model_version: str
    timestamp: str


class ModelInfo(BaseModel):
    """Informations sur un modÃ¨le"""
    name: str
    version: Optional[str]
    stage: Optional[str]
    description: Optional[str]


class HealthResponse(BaseModel):
    """RÃ©ponse du health check"""
    status: str
    model_path: str
    model_loaded: bool
    timestamp: str


# ==================== CACHE DU MODÃˆLE ====================
class ModelCache:
    """Cache simple pour Ã©viter de recharger le modÃ¨le Ã  chaque requÃªte"""
    def __init__(self):
        self.model = None
        self.model_path = None
        self.model_version = "local-docker"
        self.load_method = None
        
    def find_model_file(self):
        """Trouve le fichier modÃ¨le dans diffÃ©rents emplacements"""
        for path in MODEL_PATHS:
            if os.path.exists(path):
                if os.path.isfile(path):
                    logger.info(f"âœ… Fichier modÃ¨le trouvÃ©: {path}")
                    return path
                elif os.path.isdir(path):
                    # Chercher model.pkl dans le dossier
                    pkl_path = os.path.join(path, "model.pkl")
                    if os.path.exists(pkl_path):
                        logger.info(f"âœ… Fichier modÃ¨le trouvÃ©: {pkl_path}")
                        return pkl_path
                    # Chercher un dossier MLflow model
                    mlmodel_path = os.path.join(path, "MLmodel")
                    if os.path.exists(mlmodel_path):
                        logger.info(f"âœ… Dossier MLflow model trouvÃ©: {path}")
                        return path
                    # Lister le contenu
                    contents = os.listdir(path)
                    logger.info(f"ğŸ“ Contenu de {path}: {contents}")
        
        raise FileNotFoundError(f"Aucun modÃ¨le trouvÃ© dans: {MODEL_PATHS}")
        
    def load_model_from_file(self):
        """Charge le modÃ¨le depuis un fichier pickle local avec gestion des erreurs amÃ©liorÃ©e"""
        try:
            model_path = self.find_model_file()
            
            # Recharger seulement si changement de path
            if model_path != self.model_path:
                logger.info(f"ğŸ“‚ Chargement du modÃ¨le depuis: {model_path}")
                
                loaded = False
                errors = []
                
                # MÃ©thode 1: MLflow (pour dossiers avec MLmodel)
                if os.path.isdir(model_path) or (os.path.isdir(os.path.dirname(model_path)) and 
                    os.path.exists(os.path.join(os.path.dirname(model_path), "MLmodel"))):
                    try:
                        load_path = model_path if os.path.isdir(model_path) else os.path.dirname(model_path)
                        logger.info(f"ğŸ”„ Tentative MLflow depuis: {load_path}")
                        self.model = mlflow.sklearn.load_model(load_path)
                        self.load_method = "mlflow"
                        loaded = True
                        logger.info("âœ… ModÃ¨le chargÃ© via mlflow.sklearn.load_model")
                    except Exception as e1:
                        errors.append(f"MLflow: {str(e1)}")
                        logger.warning(f"âš ï¸ Ã‰chec MLflow: {e1}")
                
                # MÃ©thode 2: joblib avec gestion d'erreur spÃ©ciale
                if not loaded and os.path.isfile(model_path):
                    try:
                        logger.info(f"ğŸ”„ Tentative joblib depuis: {model_path}")
                        # Utiliser mmap_mode=None pour Ã©viter les problÃ¨mes de compatibilitÃ©
                        import sklearn
                        logger.info(f"Version sklearn: {sklearn.__version__}")
                        self.model = joblib.load(model_path)
                        self.load_method = "joblib"
                        loaded = True
                        logger.info("âœ… ModÃ¨le chargÃ© via joblib")
                    except Exception as e2:
                        errors.append(f"Joblib: {str(e2)}")
                        logger.warning(f"âš ï¸ Ã‰chec joblib: {e2}")
                
                # MÃ©thode 3: pickle standard
                if not loaded and os.path.isfile(model_path):
                    try:
                        logger.info(f"ğŸ”„ Tentative pickle depuis: {model_path}")
                        with open(model_path, 'rb') as f:
                            self.model = pickle.load(f)
                        self.load_method = "pickle"
                        loaded = True
                        logger.info("âœ… ModÃ¨le chargÃ© via pickle")
                    except Exception as e3:
                        errors.append(f"Pickle: {str(e3)}")
                        logger.warning(f"âš ï¸ Ã‰chec pickle: {e3}")
                
                if not loaded:
                    raise Exception(f"Toutes les mÃ©thodes ont Ã©chouÃ©: {' | '.join(errors)}")
                
                self.model_path = model_path
                logger.info(f"âœ… ModÃ¨le chargÃ© avec succÃ¨s")
                logger.info(f"   Type: {type(self.model)}")
                logger.info(f"   MÃ©thode: {self.load_method}")
                
                # Tester le modÃ¨le avec des donnÃ©es factices
                try:
                    self._test_model()
                except Exception as test_error:
                    logger.error(f"âŒ Le modÃ¨le ne fonctionne pas correctement: {test_error}")
                    raise
            
            return self.model
            
        except Exception as e:
            logger.error(f"âŒ Erreur chargement modÃ¨le: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Impossible de charger le modÃ¨le: {str(e)}"
            )
    
    def _test_model(self):
        """Teste le modÃ¨le avec des donnÃ©es factices"""
        try:
            logger.info("ğŸ§ª Test du modÃ¨le avec des donnÃ©es factices...")
            test_data = {
                "Duration in month": 12,
                "Credit amount": 5000.0,
                "Installment rate in percentage of disposable income": 2,
                "Age in years": 35,
                "Number of existing credits at this bank": 1,
                "Number of people being liable to provide maintenance for": 1,
                "Status of existing checking account": "A12",
                "Credit history": "A32",
                "Savings account/bonds": "A61",
                "Present employment since": "A73",
                "Job": "A173",
                "Purpose": "A43",
                "Personal status and sex": "A93",
                "Other debtors / guarantors": "A101",
                "Property": "A121",
                "Other installment plans": "A143",
                "Housing": "A152",
                "Telephone": "A192",
                "foreign worker": "A201"
            }
            df_test = pd.DataFrame([test_data])
            
            # Test predict
            pred = self.model.predict(df_test)
            logger.info(f"   Test predict: {pred}")
            
            # Test predict_proba
            proba = self.model.predict_proba(df_test)
            logger.info(f"   Test predict_proba: {proba}")
            
            logger.info("âœ… Test du modÃ¨le rÃ©ussi!")
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors du test: {e}")
            raise
    
    def get_model(self):
        """RÃ©cupÃ¨re le modÃ¨le (depuis cache ou charge)"""
        if self.model is None:
            self.load_model_from_file()
        return self.model, self.model_version


# Instance globale du cache
model_cache = ModelCache()


# ==================== FONCTIONS UTILITAIRES ====================
def calculate_risk_level(probability: float) -> str:
    """DÃ©termine le niveau de risque basÃ© sur la probabilitÃ©"""
    if probability >= 0.7:
        return "LOW"
    elif probability >= 0.4:
        return "MEDIUM"
    else:
        return "HIGH"


def make_prediction(model, df: pd.DataFrame) -> Dict[str, Any]:
    """Fait une prÃ©diction et retourne les rÃ©sultats formatÃ©s"""
    try:
        logger.info(f"ğŸ”® DÃ©but de la prÃ©diction...")
        logger.info(f"   Type modÃ¨le: {type(model)}")
        logger.info(f"   Shape DataFrame: {df.shape}")
        logger.info(f"   Colonnes: {df.columns.tolist()}")
        
        # PrÃ©diction
        logger.info("   Appel model.predict()...")
        prediction = model.predict(df)[0]
        logger.info(f"   âœ… PrÃ©diction: {prediction}")
        
        logger.info("   Appel model.predict_proba()...")
        probabilities = model.predict_proba(df)[0]
        logger.info(f"   âœ… ProbabilitÃ©s: {probabilities}")
        
        # Proba classe 1 (bon crÃ©dit)
        prob_good = float(probabilities[1])
        prob_bad = float(probabilities[0])
        
        result = {
            "prediction": int(prediction),
            "probability_good_credit": prob_good,
            "probability_bad_credit": prob_bad,
            "risk_level": calculate_risk_level(prob_good)
        }
        
        logger.info(f"âœ… RÃ©sultat final: {result}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la prÃ©diction: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la prÃ©diction: {str(e)}"
        )


# ==================== ENDPOINTS ====================

@app.get("/", tags=["Home"])
async def root():
    """Page d'accueil"""
    if os.path.exists("templates/index.html"):
        return FileResponse("templates/index.html")
    return JSONResponse({
        "message": "Credit Score ML API",
        "version": "2.0.0",
        "deployment": "Docker",
        "endpoints": {
            "health": "/api/health",
            "docs": "/docs",
            "predict": "/api/predict",
            "predict_batch": "/api/predict-batch",
            "model_info": "/api/model-info"
        }
    })


@app.get("/api/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """VÃ©rifie l'Ã©tat de santÃ© de l'API"""
    try:
        model, version = model_cache.get_model()
        model_loaded = model is not None
    except Exception:
        model_loaded = False
        
    return HealthResponse(
        status="healthy" if model_loaded else "degraded",
        model_path=str(model_cache.model_path) if model_cache.model_path else "N/A",
        model_loaded=model_loaded,
        timestamp=datetime.now().isoformat()
    )


@app.get("/api/model-info", tags=["Models"])
async def get_model_info():
    """RÃ©cupÃ¨re les informations du modÃ¨le chargÃ©"""
    try:
        model, version = model_cache.get_model()
        
        return {
            "model_version": version,
            "model_path": str(model_cache.model_path),
            "model_type": str(type(model)),
            "load_method": model_cache.load_method,
            "model_loaded": model is not None,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/predict", response_model=PredictionResponse, tags=["Predictions"])
async def predict_credit(application: CreditApplicationInput):
    """
    PrÃ©dit le risque de crÃ©dit pour une demande individuelle.
    
    Retourne:
    - prediction: 0 (mauvais crÃ©dit) ou 1 (bon crÃ©dit)
    - probability_good_credit: probabilitÃ© d'Ãªtre un bon crÃ©dit
    - risk_level: LOW, MEDIUM, ou HIGH
    """
    try:
        # Charger le modÃ¨le
        model, version = model_cache.get_model()
        
        # Convertir en DataFrame
        df = application.to_dataframe()
        
        logger.info(f"ğŸ“¥ Nouvelle requÃªte de prÃ©diction")
        logger.info(f"   DonnÃ©es: {df.iloc[0].to_dict()}")
        
        # Faire la prÃ©diction
        result = make_prediction(model, df)
        
        # CrÃ©er la rÃ©ponse
        response = PredictionResponse(
            **result,
            model_version=str(version),
            timestamp=datetime.now().isoformat()
        )
        
        logger.info(f"ğŸ“¤ RÃ©sultat: {result['prediction']} (proba: {result['probability_good_credit']:.3f})")
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la prÃ©diction: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la prÃ©diction: {str(e)}"
        )


@app.post("/api/predict-batch", response_model=BatchPredictionResponse, tags=["Predictions"])
async def predict_batch(request: BatchPredictionRequest):
    """
    PrÃ©dit le risque de crÃ©dit pour plusieurs demandes en batch.
    
    Accepte une liste de demandes de crÃ©dit et retourne les prÃ©dictions
    pour chacune.
    """
    try:
        # Charger le modÃ¨le
        model, version = model_cache.get_model()
        
        # Convertir toutes les applications en DataFrame
        dfs = [app.to_dataframe() for app in request.applications]
        df_batch = pd.concat(dfs, ignore_index=True)
        
        logger.info(f"ğŸ“¥ PrÃ©diction batch pour {len(request.applications)} demandes")
        
        # Faire les prÃ©dictions
        predictions_list = []
        for i, row in df_batch.iterrows():
            df_single = pd.DataFrame([row])
            result = make_prediction(model, df_single)
            
            predictions_list.append(
                PredictionResponse(
                    **result,
                    model_version=version,
                    timestamp=datetime.now().isoformat()
                )
            )
        
        response = BatchPredictionResponse(
            predictions=predictions_list,
            total_processed=len(predictions_list),
            model_version=str(version),
            timestamp=datetime.now().isoformat()
        )
        
        logger.info(f"âœ… Batch terminÃ©: {len(predictions_list)} prÃ©dictions")
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur batch: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors du batch: {str(e)}"
        )


@app.post("/api/reload-model", tags=["Admin"])
async def reload_model():
    """Force le rechargement du modÃ¨le depuis le disque"""
    try:
        model_cache.model = None
        model_cache.model_path = None
        
        model, version = model_cache.get_model()
        
        return {
            "message": "ModÃ¨le rechargÃ© avec succÃ¨s",
            "model_path": str(model_cache.model_path),
            "version": version,
            "load_method": model_cache.load_method,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors du rechargement: {str(e)}"
        )


# ==================== GESTION DES ERREURS ====================
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Gestion globale des erreurs"""
    logger.error(f"âŒ Erreur non gÃ©rÃ©e: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "detail": "Une erreur interne est survenue",
            "error": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )


# ==================== STARTUP ====================
@app.on_event("startup")
async def startup_event():
    """Actions au dÃ©marrage de l'API"""
    logger.info("="*60)
    logger.info("ğŸš€ DÃ‰MARRAGE DE L'API CREDIT SCORE ML (DOCKER)")
    logger.info("="*60)
    
    # Lister tous les chemins possibles
    logger.info("ğŸ“‚ Chemins de recherche du modÃ¨le:")
    for path in MODEL_PATHS:
        exists = "âœ…" if os.path.exists(path) else "âŒ"
        logger.info(f"   {exists} {path}")
    
    # Lister le contenu de /app
    if os.path.exists("/app"):
        logger.info("\nğŸ“ Contenu de /app:")
        for item in os.listdir("/app"):
            item_path = os.path.join("/app", item)
            if os.path.isdir(item_path):
                logger.info(f"   ğŸ“‚ {item}/")
                # Lister aussi le contenu des sous-dossiers
                try:
                    sub_items = os.listdir(item_path)
                    for sub_item in sub_items[:5]:  # Limiter Ã  5 items
                        logger.info(f"      - {sub_item}")
                    if len(sub_items) > 5:
                        logger.info(f"      ... et {len(sub_items)-5} autres")
                except:
                    pass
            else:
                logger.info(f"   ğŸ“„ {item}")
    
    logger.info("\nğŸ”„ Tentative de chargement du modÃ¨le...")
    try:
        # PrÃ©charger le modÃ¨le
        model, version = model_cache.get_model()
        logger.info(f"âœ… ModÃ¨le prÃ©chargÃ© avec succÃ¨s!")
        logger.info(f"   Version: {version}")
        logger.info(f"   MÃ©thode: {model_cache.load_method}")
        logger.info(f"   Chemin: {model_cache.model_path}")
    except Exception as e:
        logger.error(f"âŒ Impossible de prÃ©charger le modÃ¨le: {e}")
        logger.warning("âš ï¸ L'API dÃ©marrera sans modÃ¨le - il sera chargÃ© Ã  la premiÃ¨re requÃªte")
    
    logger.info("="*60)


# ==================== LANCEMENT ====================
if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*60)
    print("ğŸš€ DÃ‰MARRAGE DE L'API CREDIT SCORE ML (DOCKER)")
    print("="*60)
    print(f"ğŸ“ URL: http://localhost:8000")
    print(f"ğŸ“š Documentation: http://localhost:8000/docs")
    print(f"ğŸ“‚ Model Paths: {MODEL_PATHS}")
    print("="*60 + "\n")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )