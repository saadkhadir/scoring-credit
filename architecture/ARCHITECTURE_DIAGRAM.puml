@startuml Credit_Score_ML_Architecture
!theme plain
skinparam backgroundColor #FFFFFF
skinparam defaultTextAlignment center
skinparam rectangle {
    BackgroundColor #E8F4F8
    BorderColor #0369A1
    FontColor #1E293B
    BorderThickness 2
}
skinparam component {
    BackgroundColor #F0F9FF
    BorderColor #0284C7
    FontColor #1E293B
    BorderThickness 2
}
skinparam database {
    BackgroundColor #FEF3C7
    BorderColor #D97706
    FontColor #1E293B
    BorderThickness 2
}

title Credit Score ML API - Architecture Abstraite\n\n

' ==================== DEV ENVIRONMENT ====================
rectangle "ðŸŸ¢ DEV ENVIRONMENT" as dev_env {
    
    component "ðŸ“š MLflow Tracking\nExperiment Logging\nMetrics Storage" as mlflow_tracking
    
    rectangle "ðŸ¤– Model" as model_dev {
        component "scikit-learn\nRandomForest\nClassifier" as model_box
        component "Environment:\nPython 3.13.2" as env_box
    }
    
    component "ðŸ“‹ MLflow Model Registry\nModel Versioning\nStaging Management" as mlflow_registry
}

' ==================== DOCKER CONTAINER ====================
rectangle "ðŸ³ DOCKER CONTAINER (Orchestration)" as docker_layer {
    component "$ mlflow deployments" as mlflow_cmd
    component "$ mlflow models serve" as models_serve_cmd
}

' ==================== PRODUCTION ENVIRONMENT ====================
rectangle "ðŸ”´ PRODUCTION ENVIRONMENT" as prod_env {
    
    ' Frontend & API
    rectangle "ðŸŽ¨ Frontend & API" as frontend_api {
        component "HTML/CSS/JavaScript\n+ FastAPI\n+ Pydantic\n+ CORS Middleware" as frontend_api_tech
    }
    
    ' ML Services
    component "â˜ï¸ Cloud ML Services\n(Optional)\n- Databricks\n- Amazon Sagemaker\n- Azure ML\n- Kubernetes Cluster" as cloud_services
    
    ' Monitoring
    component "ðŸ“Š Monitoring & Logging\n- Prometheus\n- Grafana\n- prometheus-fastapi-instrumentator" as monitoring_stack
}

' ==================== LOCAL INFERENCE ====================
rectangle "ðŸ”µ LOCAL INFERENCE (Single Prediction)" as local_inference {
    component "ðŸ Flask/FastAPI Server\nLocal Model Loading\nPreprocessing Pipeline" as local_server
    
    component "ðŸ“„ Batch Production\nBulk Predictions\nCSV Output" as batch_processing
}

' ==================== STORAGE & DATA ====================
rectangle "ðŸ’¾ DATA STORAGE" as data_storage {
    database "MLflow Database\nsqlite:///mlflow.db" as mlflow_db
    database "Training Data\nestadistical.csv\n(1000 samples, 21 features)" as training_data
    database "Model Artifacts\nmodel.pkl\nPreprocessor + Classifier" as model_artifacts
}

' ==================== CONNECTIONS ====================

' Training Flow
mlflow_tracking --> model_box: "1ï¸âƒ£ Track & Train"
model_box --> mlflow_registry: "2ï¸âƒ£ Register Model"
mlflow_registry --> model_artifacts: "ðŸ’¾ Save Artifacts"

' Deployment Flow
mlflow_registry --> mlflow_cmd: "3ï¸âƒ£ Deploy"
mlflow_registry --> models_serve_cmd: "Deploy"
mlflow_cmd --> docker_layer: "Containerize"
models_serve_cmd --> docker_layer: "Containerize"

' Docker to Production
docker_layer --> frontend_api: "4ï¸âƒ£ Load Model"
docker_layer --> cloud_services: "OR Deploy to Cloud"
docker_layer --> local_server: "5ï¸âƒ£ OR Deploy Locally"

' Production Flow
frontend_api --> monitoring_stack: "6ï¸âƒ£ Monitor Predictions"
local_server --> batch_processing: "Process Batch"

' Data Loading
model_artifacts -.-> frontend_api: "Load Model"
model_artifacts -.-> local_server: "Load Model"
training_data -.-> mlflow_tracking: "Training Source"

' Storage Connections
mlflow_tracking -.-> mlflow_db: "Log Metrics"
mlflow_registry -.-> mlflow_db: "Registry DB"

' ==================== LEGEND ====================
note bottom of dev_env
  **Development Phase:**
  - Train model with script.py
  - Log experiments with MLflow
  - Register model in MLflow Registry
  - Store artifacts (pickle files)
end note

note bottom of prod_env
  **Production Phase:**
  - Deploy to Docker container
  - Optional: Deploy to cloud (AWS, Azure, Databricks)
  - Monitor with Prometheus + Grafana
  - API: FastAPI on port 8000
end note

note bottom of local_inference
  **Inference:**
  - Single prediction: REST API call
  - Batch prediction: Process multiple records
  - Local deployment: No cloud required
end note

@enduml
