@startuml Credit_Score_ML_DataFlow
!theme plain
skinparam backgroundColor #FFFFFF
skinparam defaultTextAlignment center
skinparam rectangle {
    BackgroundColor #E8F4F8
    BorderColor #0369A1
    FontColor #1E293B
    BorderThickness 2
}
skinparam component {
    BackgroundColor #F0F9FF
    BorderColor #0284C7
    FontColor #1E293B
    BorderThickness 2
}
skinparam database {
    BackgroundColor #FEF3C7
    BorderColor #D97706
    FontColor #1E293B
    BorderThickness 2
}

title Credit Score ML - Flux de DonnÃ©es et Pipeline ML\n\n

' ==================== TRAINING PHASE ====================
rectangle "ðŸ“š PHASE D'ENTRAÃŽNEMENT (Training)" as training_phase {
    
    rectangle "ðŸ“Š Data Loading & Preparation" as data_prep {
        database "Source: estadistical.csv\n1000 samples\n21 features\nTarget: Receive/Not receive credit" as source_data
        component "Pandas DataFrame\nLoad & Clean" as pandas_load
        component "Column Normalization\nRename: 'Receive/Not receive credit'\nâ†’ 'Credit_Risk'" as column_norm
        component "Target Mapping\n2 (Bad) â†’ 0\n1 (Good) â†’ 1" as target_mapping
        component "Train/Test Split\n70% train, 30% test\nStratified split" as train_test_split
    }
    
    rectangle "ðŸ”„ Feature Engineering" as feature_eng {
        component "Numerical Features\n(6 features)\n- Duration in month\n- Credit amount\n- Installment rate\n- Age in years\n- Num existing credits\n- Num dependents" as num_features
        
        component "Ordinal Categorical\n(5 features)\n- Status checking account\n- Credit history\n- Savings account\n- Employment since\n- Job" as ordinal_features
        
        component "Nominal Categorical\n(8 features)\n- Purpose\n- Personal status & sex\n- Other debtors\n- Property\n- Other installment plans\n- Housing\n- Telephone\n- Foreign worker" as nominal_features
        
        component "CustomPreprocessor:\n1. Ordinal Mapping (A11â†’0..A14â†’3)\n2. One-Hot Encoding\n3. StandardScaler" as preprocessor_logic
    }
    
    rectangle "ðŸ¤– Model Training" as model_training {
        component "Pipeline Creation:\nscikit-learn Pipeline\n- Preprocessor step\n- Classifier step" as pipeline_creation
        
        component "RandomForest Configuration:\n- n_estimators=100\n- max_depth=10\n- min_samples_split=20\n- min_samples_leaf=10\n- random_state=42" as rf_config
        
        component "Model Fitting:\nX_train, y_train\nFit preprocessing + classifier" as model_fit
        
        component "Training Output:\nTrained Pipeline object\nReady for predictions" as training_output
    }
    
    rectangle "ðŸ“ˆ Model Evaluation" as evaluation {
        component "Predictions on Test Set\ny_pred, y_pred_proba" as test_predictions
        
        component "Metrics Calculation:\n- Accuracy Score\n- Classification Report\n  (Precision, Recall, F1)\n- Confusion Matrix" as metrics_calc
        
        component "Evaluation Results\n(Example: Accuracy=0.85)" as eval_results
    }
    
    rectangle "ðŸ’¾ MLflow Logging" as mlflow_logging {
        component "MLflow Tracking\nExperiment: credit-score-production\nRun: rf-pipeline-v2" as mlflow_run
        
        component "Log Artifacts:\n- classification_report.txt\n- confusion_matrix.txt\n- model_metadata.json" as mlflow_artifacts
        
        component "Log Model:\nsk_model=pipeline\nartifact_path='model'\nregistered_model_name=\n'RDF_score_pipeline'" as mlflow_model_log
        
        component "Model Signature:\ninfer_signature(X_train, predictions)\nDefines input/output schema" as model_signature
        
        database "MLflow Database\nsqlite:///mlflow.db\n- Run metadata\n- Metrics\n- Parameters\n- Artifacts location" as mlflow_db
    }
    
    rectangle "ðŸŽ¯ Model Promotion" as model_promotion {
        component "MLflow Model Registry\nModel: RDF_score_pipeline" as model_registry
        
        component "Version Management\nCreate version\nAssign version number" as version_mgmt
        
        component "Stage Transition\nStaging â†’ Production\nArchive old Production versions" as stage_transition
        
        database "Artifact Storage\n/mlruns/1/models/m-.../artifacts/\n- conda.yaml\n- python_env.yaml\n- MLmodel\n- requirements.txt\n- model.pkl" as artifact_storage
    }
}

' ==================== INFERENCE PHASE ====================
rectangle "ðŸ”® PHASE D'INFÃ‰RENCE (Serving)" as inference_phase {
    
    rectangle "ðŸ“¥ Request Reception" as request_reception {
        actor "Client/User" as client_user
        component "HTTP POST Request\nJSON Payload\nCreditApplicationInput\nschema validation" as http_request
        
        component "Pydantic Validation\n- Type checking\n- Range validation\n- Field aliases\n- Error handling" as pydantic_validation
        
        component "Validated DataFrame\nOriginal column names\nNumPy types" as validated_df
    }
    
    rectangle "ðŸ”„ Data Preprocessing (Inference)" as inference_preprocessing {
        component "Same CustomPreprocessor\ninstance used in training\n(Fitted scaler state)" as preprocessor_instance
        
        component "1. Ordinal Mapping\nApply learned mappings\nE.g., 'A12' â†’ 1" as inf_ordinal
        
        component "2. One-Hot Encoding\nApply training categories\nFill missing with 0" as inf_onehot
        
        component "3. StandardScaler\nApply training statistics\nmean/std from training" as inf_scaler
        
        component "Preprocessed Features\nNumpy array\nSame shape as training" as preprocessed_features
    }
    
    rectangle "ðŸ¤– Model Prediction" as inference_prediction {
        component "Load Model from Cache\nor MLflow Registry\n(Production version)" as model_load
        
        component "predict(X)\nTree traversal\nClass prediction (0 or 1)" as predict_method
        
        component "predict_proba(X)\nClass probability [P_bad, P_good]\nRange: [0, 1]" as predict_proba
        
        component "Risk Level Calculation\nP_good â‰¥ 0.7 â†’ LOW\nP_good â‰¥ 0.4 â†’ MEDIUM\nP_good < 0.4 â†’ HIGH" as risk_calc
    }
    
    rectangle "ðŸ“¤ Response Building" as response_building {
        component "PredictionResponse Object:\n- prediction (0 or 1)\n- probability_good_credit\n- probability_bad_credit\n- risk_level (LOW/MEDIUM/HIGH)\n- model_version\n- timestamp" as response_object
        
        component "JSON Serialization\nFastAPI JSONResponse\nHTTP Status 200" as json_response
    }
    
    rectangle "ðŸ“Š Metrics Recording" as metrics_recording {
        component "Prometheus Counters:\n- credit_predictions_total\n- credit_predictions_good_total\n- credit_predictions_bad_total\n- api_request_errors_total" as prom_counters
        
        component "Prometheus Histograms:\n- credit_prediction_duration_seconds\nBuckets: 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0" as prom_histograms
        
        component "Expose /metrics endpoint\nPrometheus scrapes every 15s" as metrics_expose
    }
}

' ==================== BATCH INFERENCE ====================
rectangle "ðŸ“¦ BATCH INFERENCE (Optional)" as batch_inference {
    component "POST /api/predict-batch\nBatchPredictionRequest\nList[CreditApplicationInput]" as batch_request
    
    component "Loop Processing:\nFor each application\n- Validate\n- Preprocess\n- Predict" as batch_loop
    
    component "BatchPredictionResponse\nList[PredictionResponse]\n+ total_processed\n+ model_version\n+ timestamp" as batch_response
}

' ==================== MONITORING & DASHBOARDS ====================
rectangle "ðŸ“Š MONITORING (Grafana/Prometheus)" as monitoring {
    component "Time-Series Metrics\nStored in Prometheus\nRetained: 30 days" as timeseries
    
    component "Grafana Dashboard:\nCredit Score Dashboard\n- Prediction volume\n- Distribution (Good/Bad)\n- Average latency\n- Error rates" as grafana_viz
    
    component "Real-time Alerts\n(Configurable thresholds)\nError rate > 5%\nLatency > 1s" as alerts
}

' ==================== CONNECTIONS: TRAINING PHASE ====================
source_data --> pandas_load: "Load CSV"
pandas_load --> column_norm: "Normalize"
column_norm --> target_mapping: "Map Target"
target_mapping --> train_test_split: "Split Data"

train_test_split --> num_features: "Numerical"
train_test_split --> ordinal_features: "Ordinal"
train_test_split --> nominal_features: "Nominal"

num_features --> preprocessor_logic: "Combine"
ordinal_features --> preprocessor_logic: "Combine"
nominal_features --> preprocessor_logic: "Combine"

preprocessor_logic --> pipeline_creation: "Integrate"
rf_config --> pipeline_creation: "Configure"
pipeline_creation --> model_fit: "Create"
model_fit --> training_output: "Output"

training_output --> test_predictions: "Evaluate"
test_predictions --> metrics_calc: "Calculate"
metrics_calc --> eval_results: "Results"

eval_results --> mlflow_run: "Start run"
mlflow_run --> mlflow_artifacts: "Log artifacts"
mlflow_run --> mlflow_model_log: "Log model"
mlflow_model_log --> model_signature: "Create signature"

mlflow_run --> mlflow_db: "Store metadata"
mlflow_artifacts --> mlflow_db: "Reference"

mlflow_model_log --> model_registry: "Register"
model_registry --> version_mgmt: "Version"
version_mgmt --> stage_transition: "Promote"
stage_transition --> artifact_storage: "Store artifacts"

' ==================== CONNECTIONS: INFERENCE PHASE ====================
client_user --> http_request: "Send request"
http_request --> pydantic_validation: "Validate"
pydantic_validation --> validated_df: "Convert"

validated_df --> preprocessor_instance: "Preprocess"
preprocessor_instance --> inf_ordinal: "Step 1"
inf_ordinal --> inf_onehot: "Step 2"
inf_onehot --> inf_scaler: "Step 3"
inf_scaler --> preprocessed_features: "Output"

artifact_storage -.-> model_load: "Serve model"
model_load --> predict_method: "Predict"
predict_method --> predict_proba: "Get proba"
preprocessed_features --> predict_method: "Input"
predict_proba --> risk_calc: "Calculate"

predict_method --> response_object: "Build response"
predict_proba --> response_object: "Populate"
risk_calc --> response_object: "Add risk level"

response_object --> json_response: "Serialize"
json_response --> client_user: "Return HTTP 200"

predict_method --> metrics_recording: "Latency"
predict_proba --> metrics_recording: "Count"
metrics_recording --> prom_counters: "Record"
prom_counters --> prom_histograms: "Aggregate"
prom_histograms --> metrics_expose: "Expose"

' ==================== CONNECTIONS: BATCH ====================
batch_request --> batch_loop: "Process"
batch_loop --> batch_response: "Collect results"

' ==================== CONNECTIONS: MONITORING ====================
prom_counters --> timeseries: "Send metrics"
prom_histograms --> timeseries: "Send metrics"
metrics_expose -.-> timeseries: "Scrape"
timeseries --> grafana_viz: "Query"
grafana_viz --> alerts: "Alert"

' ==================== LEGEND ====================
note top of training_phase
  **PHASE DE TRAINING (Offline)**
  ExÃ©cutÃ©e une seule fois ou pÃ©riodiquement
  Produit le modÃ¨le sauvegardÃ© dans MLflow
  script.py orchestre tout le processus
end note

note top of inference_phase
  **PHASE D'INFÃ‰RENCE (Production)**
  API main.py utilise le modÃ¨le sauvegardÃ©
  Chaque requÃªte suit ce chemin
  MÃ©triques collectÃ©es pour le monitoring
end note

note bottom of preprocessor_logic
  **Key Point**: Same preprocessor instance
  must be used in training AND inference
  to ensure consistency
end note

note right of artifact_storage
  **Model Serving Strategy**
  - Load model from MLflow Registry
  - Cache in memory (ModelCache)
  - Use fitted preprocessing pipeline
  - Reuse scaler statistics
end note

note bottom of metrics_recording
  **Prometheus Integration**
  Automatically scraped by Prometheus
  Visualized in Grafana dashboards
  Used for alerting & monitoring
end note

@enduml
